{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "from itertools import combinations\n",
    "import logging\n",
    "from random import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from aeda import utils as u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(logging)\n",
    "FORMAT = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(filename='pk-search-mysql.log', level=logging.INFO, format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger('sqlalchemy').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = 'mysql-demo'\n",
    "# configuration = 'sqlserver-sales'\n",
    "# configuration = 'sqlserver-production'\n",
    "conn_string = u.get_db_connection_string(configuration)\n",
    "connection = u.get_db_connection(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection = source_engine.connect()\n",
    "\n",
    "# table_catalog = 'def'\n",
    "# table_schema = 'world'\n",
    "\n",
    "table_catalog = conn_string['catalog']\n",
    "table_schema = conn_string['schema']\n",
    "\n",
    "sql = \"\"\"SELECT C.TABLE_CATALOG\n",
    "                , C.TABLE_SCHEMA\n",
    "                , C.TABLE_NAME\n",
    "                , C.COLUMN_NAME\n",
    "                , C.ORDINAL_POSITION\n",
    "                , C.DATA_TYPE\n",
    "            FROM INFORMATION_SCHEMA.COLUMNS AS C INNER JOIN INFORMATION_SCHEMA.TABLES AS T\n",
    "            ON C.TABLE_CATALOG = T.TABLE_CATALOG\n",
    "            AND C.TABLE_SCHEMA = T.TABLE_SCHEMA\n",
    "            AND C.TABLE_NAME = T.TABLE_NAME\n",
    "            AND T.TABLE_TYPE = 'BASE TABLE'\n",
    "            AND T.TABLE_CATALOG = '{}'\n",
    "            AND T.TABLE_SCHEMA = '{}';\"\"\".format(table_catalog, table_schema)\n",
    "\n",
    "df_tables = pd.read_sql_query(sql, connection)\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_list = []\n",
    "for _, t in df_tables.iterrows():\n",
    "    if t.TABLE_NAME not in table_list:\n",
    "        table_list.append(t.TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(table_list)\n",
    "table_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_include = []\n",
    "\n",
    "with open('tables_processed.txt', 'r') as f:\n",
    "    processed = f.read()\n",
    "    \n",
    "processed_list = []\n",
    "for table in processed.split():\n",
    "    processed_list.append(table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_from_table(owner, table_name):\n",
    "    columns = []\n",
    "    for _, r in df_tables[df_tables['TABLE_NAME'] == table_name].iterrows():\n",
    "        columns.append(r.COLUMN_NAME)\n",
    "    logger.info('{} has {} columns'.format(table_name, len(columns)))\n",
    "    return columns\n",
    "\n",
    "# Global Test Datasets\n",
    "df_level_1 = pd.DataFrame()\n",
    "df_level_2 = pd.DataFrame()\n",
    "df_level_3 = pd.DataFrame()\n",
    "\n",
    "def clean_test_datasets(table_name):\n",
    "    global df_level_1, df_level_2, df_level_3\n",
    "    logger.info('{} Reseting dataframe samples'.format(table_name))\n",
    "    df_level_1 = pd.DataFrame()\n",
    "    df_level_2 = pd.DataFrame()\n",
    "    df_level_3 = pd.DataFrame()\n",
    "    return\n",
    "\n",
    "def get_sql_sample(table_name, columns, top_n):\n",
    "    if conn_string[\"db_engine\"] == 'mysql':\n",
    "        sql = \"\"\"select {} from {} LIMIT {};\"\"\".format(', '.join([str('`{}`').format(c) for c in columns]), table_name, top_n)\n",
    "    elif conn_string[\"db_engine\"] == 'mssqlserver':\n",
    "        sql = \"\"\"select top {} * from {}.{};\"\"\".format(top_n, conn_string['schema'] , table_name)\n",
    "    # sql = 'SELECT * FROM {}.{} LIMIT {};'.format(table_name, columns, top_n)\n",
    "    #sql = \"\"\"select * from {} where ROWNUM <= {}\"\"\".format(table_name, top_n)\n",
    "    #sql = \"\"\"select {} from {} where ROWNUM <= {}\"\"\".format(', '.join(columns), table_name, top_n)\n",
    "    #sql = \"\"\"select {} from {} LIMIT {};\"\"\".format(', '.join(columns), table_name, top_n)\n",
    "    \n",
    "    return sql\n",
    "\n",
    "def get_df_sql(sql, connection):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with the results of a query.\n",
    "    Used to execute evaluations of the PKs on a subsample of the data.\n",
    "    \"\"\"\n",
    "    conn_string = u.get_db_connection_string(configuration)\n",
    "    connection = u.get_db_connection(conn_string)\n",
    "    df = pd.read_sql_query(sql, connection)\n",
    "    connection.close()\n",
    "    return df\n",
    "\n",
    "def create_testing_datasets(owner, table_name, columns, n1=10_000, n2=100_000, n3=500_000):\n",
    "    global df_level_1, df_level_2, df_level_3\n",
    "    \n",
    "    logger.info('{} Creating a level 1 dataset sample with {:,} rows'.format(table_name, n1))\n",
    "    sql = get_sql_sample(table_name, columns, n1)\n",
    "    df_level_1 = get_df_sql(sql, connection) \n",
    "    logger.info('{} Level 1 dataset sample created'.format(table_name))\n",
    "    \n",
    "    if len(df_level_1) < n1:\n",
    "        logger.info('{} Level 1 dataset sample  has {:,} rows, no need for more testing datasets'.format(table_name, len(df_level_1)))\n",
    "        df_level_2 = df_level_1\n",
    "        df_level_3 = df_level_1\n",
    "        return \n",
    "    else:\n",
    "        logger.info('{} Creating a level 2 dataset sample with {:,} rows'.format(table_name, n2))\n",
    "        sql = get_sql_sample(table_name, columns, n2)\n",
    "        df_level_2 = get_df_sql(sql, connection) \n",
    "        logger.info('{} Level 2 dataset sample created'.format(table_name))\n",
    "        \n",
    "        if len(df_level_2) < n2:\n",
    "            logger.info('{} Level 2 dataset sample has {:,} rows, no need for more testing datasets'.format(table_name, len(df_level_2)))\n",
    "            df_level_3 = df_level_2\n",
    "            return \n",
    "        else:\n",
    "            logger.info('{} Creating a level 3 dataset sample with {:,} rows'.format(table_name, n3))\n",
    "            sql = get_sql_sample(table_name, columns, n3)\n",
    "            df_level_3 = get_df_sql(sql, connection)\n",
    "            logger.info('{} Level 3 dataset sample created'.format(table_name))\n",
    "            \n",
    "            if len(df_level_3) < n3:\n",
    "                logger.info('{} Level 3 dataset sample has {:,} rows, no need for testing against the database'.format(table_name, len(df_level_3)))\n",
    "                return \n",
    "    return\n",
    "\n",
    "def get_column_combinations(columns, k = 5):\n",
    "    \"\"\"\n",
    "    Returns possible combinations of the columns.\n",
    "    \"\"\"\n",
    "    results = combinations(columns, k)\n",
    "    return results\n",
    "\n",
    "def count_iterable(i):\n",
    "    \"\"\"\n",
    "    Returns the number of elements in an iterable.\n",
    "    Used to get the number of combinations to test.\n",
    "    \"\"\"\n",
    "    return sum(1 for e in i)\n",
    "\n",
    "def get_all_candidates_from_df(table_name, columns, number_of_columns, threshold):\n",
    "    hundred_percent_counter = 0\n",
    "    all_candidates = []\n",
    "    \n",
    "    if not (hundred_percent_counter > 0) and len(columns) > 0 and len(df_level_1) > 0:\n",
    "        logger.info('{} Searching candidate keys in combinations of {} columns'.format(table_name, number_of_columns))\n",
    "\n",
    "        candidates_level_1 = []\n",
    "        candidates_level_2 = []\n",
    "        candidates_level_3 = []\n",
    "\n",
    "        tot_combinations = count_iterable(get_column_combinations(columns, number_of_columns))\n",
    "        column_combinations = get_column_combinations(columns, number_of_columns)\n",
    "\n",
    "        logger.info('{} Level 1 dataset: searching {:,} candidates on {:,} records'.format(table_name, tot_combinations, df_level_1.shape[0]))\n",
    "        for item in tqdm(column_combinations, total=tot_combinations, unit='checks'):\n",
    "            records = len(df_level_1.groupby(list(item)).size().reset_index(name='Freq'))\n",
    "            if records / df_level_1.shape[0] >= threshold:\n",
    "                candidates_level_1.append(item)\n",
    "        logger.info('{} Level 1 dataset: {:,} candidates out of {:,} combinations tested in {:,} records'.format(table_name, len(candidates_level_1), tot_combinations, df_level_1.shape[0]))\n",
    "\n",
    "        if len(candidates_level_1) > 0:\n",
    "            logger.info('{} Level 2 dataset: searching {:,} candidates on {:,} records'.format(table_name, len(candidates_level_1), df_level_2.shape[0]))\n",
    "            candidates_level_2 = []\n",
    "            for candidate in tqdm(candidates_level_1):\n",
    "                records_100k = len(df_level_2.groupby(list(candidate)).size().reset_index(name='Freq'))\n",
    "                if records_100k / df_level_2.shape[0] > threshold:\n",
    "                    candidates_level_2.append(candidate)\n",
    "            logger.info('{} Level 2 dataset: {:,} candidates out of {:,} posibilities tested in {:,} records'.format(table_name, len(candidates_level_2), len(candidates_level_1), df_level_2.shape[0]))\n",
    "\n",
    "        if len(candidates_level_2) > 0:\n",
    "            logger.info('{} Level 3 dataset: searching {:,} candidates on {:,} records'.format(table_name, len(candidates_level_2), df_level_3.shape[0]))\n",
    "            candidates_level_3 = []\n",
    "            for candidate in tqdm(candidates_level_2):\n",
    "                records_1M = len(df_level_3.groupby(list(candidate)).size().reset_index(name='Freq'))\n",
    "                if records_1M / df_level_3.shape[0] > threshold:\n",
    "                    logger.info('{} Candidate: {} Unique: {:,} Percentage: {:.5%}'.format(table_name, candidate, records_1M, records_1M / df_level_3.shape[0]))\n",
    "                    candidates_level_3.append(candidate)\n",
    "                    all_candidates.append(candidate)\n",
    "                    if records_1M / df_level_3.shape[0] == 1:\n",
    "                        hundred_percent_counter+=1\n",
    "            logger.info('{} Level 3 dataset: {:,} candidates out of {:,} posibilities tested in {:,} records'.format(table_name, len(candidates_level_3), len(candidates_level_2), df_level_3.shape[0]))\n",
    "    return all_candidates\n",
    "\n",
    "def get_sql(fields, table_name, top_n=10_000):\n",
    "    if isinstance(fields, list) or isinstance(fields, tuple):\n",
    "        if top_n > 0:\n",
    "            sql = \"\"\"select count(distinct concat({})) as n1, count(*) as n2 from (select top {} * from {}) as t;\"\"\".format(', '.join(fields), top_n, table_name)\n",
    "        else:\n",
    "            if conn_string[\"db_engine\"] == 'mssqlserver':\n",
    "                if len(fields) > 1:\n",
    "                    sql = \"\"\"select count(distinct concat({})) as n1, count(*) as n2 from {}\"\"\".format(', '.join(fields), table_name)\n",
    "                else:\n",
    "                    sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from {}\"\"\".format(fields[0], table_name)\n",
    "                    # sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from {}.{}\"\"\".format(', '.join([str('[{}]').format(x) for x in fields]), conn_string[\"schema\"], table_name)\n",
    "            elif conn_string[\"db_engine\"] == 'mysql':\n",
    "                sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from {}.{}\"\"\".format(', '.join([str('`{}`').format(x) for x in fields]), conn_string[\"schema\"], table_name)\n",
    "            else:\n",
    "                sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from {};\"\"\".format(', '.join(fields), table_name)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        if top_n > 0:\n",
    "            sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from (select top {} * from {}) as t;\"\"\".format(fields, top_n, table_name)\n",
    "        else:\n",
    "            if conn_string[\"db_engine\"] == 'mssqlserver':\n",
    "                sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from {}.{}\"\"\".format(', '.join([str('{}').format(x) for x in fields]), conn_string[\"schema\"] , table_name)\n",
    "            elif conn_string[\"db_engine\"] == 'mysql':\n",
    "                sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from {}.{}\"\"\".format(', '.join([str('`{}`').format(x) for x in fields]), conn_string[\"schema\"] , table_name)\n",
    "            else:\n",
    "                sql = \"\"\"select count(distinct {}) as n1, count(*) as n2 from {};\"\"\".format(fields, table_name)\n",
    "            \n",
    "            # \n",
    "    return sql\n",
    "\n",
    "def run_query_on_source(query):\n",
    "    \"\"\"\n",
    "    Execute query on source server and returns the result.\n",
    "    This shold be used only for queries that returns 1 row such as counts or summaries for \n",
    "    performance reasons.\n",
    "    \"\"\"\n",
    "    conn_string = u.get_db_connection_string(configuration)\n",
    "    connection = u.get_db_connection(conn_string)\n",
    "    df = pd.read_sql_query(query, connection)\n",
    "    connection.close()\n",
    "    return df\n",
    "\n",
    "def get_final_candidates_from_db(table_name, all_candidates, threshold):\n",
    "    final_candidates = []\n",
    "    for candidate in tqdm(all_candidates, unit='candidates'):\n",
    "        sql = get_sql(candidate, table_name, 0)\n",
    "        rows = run_query_on_source(sql)\n",
    "        if rows.iloc[0].n1 / rows.iloc[0].n2 > threshold:\n",
    "            logger.info('{} In all data: {} columns candidate {} with {:.5%} of unique values or {:,} in {:,} records'.format(table_name, len(candidate), candidate, rows.iloc[0].n1 / rows.iloc[0].n2, rows.iloc[0].n1, rows.iloc[0].n2))\n",
    "            final_candidates.append(candidate)\n",
    "    return final_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(owner, table_name, threshold = 0.99999):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    - hundred_percent_counter can be 1 after 1M but could not make it in the full table, need to return to processing more \n",
    "    columns.\n",
    "    \"\"\"\n",
    "    columns = get_columns_from_table(owner, table_name)\n",
    "    clean_test_datasets(table_name)\n",
    "    if len(columns) > 0:\n",
    "        create_testing_datasets(owner, table_name, columns, n1=100, n2=10_000,n3=100_000)\n",
    "    \n",
    "    threshold = threshold\n",
    "    \n",
    "    MAX_COLUMNS = 5\n",
    "    levels_threshold = [0.99999, 0.98]\n",
    "    all_candidates = []\n",
    "    final_candidates = []\n",
    "    \n",
    "    number_of_columns = 1\n",
    "    while len(final_candidates) == 0 and number_of_columns <= MAX_COLUMNS and len(columns) > 0 and len(df_level_1) > 0:\n",
    "        if len(df_level_1) > 0:\n",
    "            all_candidates = get_all_candidates_from_df(table_name, columns, number_of_columns, threshold)\n",
    "            final_candidates = get_final_candidates_from_db(table_name, all_candidates, threshold)\n",
    "\n",
    "            if number_of_columns == MAX_COLUMNS and len(final_candidates) == 0 and threshold > 0.98:\n",
    "                number_of_columns = 1\n",
    "                threshold = threshold * 0.99\n",
    "                logger.info('{} threshold in: {:.5%}'.format(table_name, threshold))\n",
    "            elif number_of_columns < MAX_COLUMNS and len(final_candidates) == 0:\n",
    "                number_of_columns +=1\n",
    "            elif len(all_candidates) > 0 and len(final_candidates) == 0:\n",
    "                threshold = threshold * 0.99\n",
    "                logger.info('{} threshold in: {:.5%}'.format(table_name, threshold))\n",
    "            elif number_of_columns == MAX_COLUMNS and len(final_candidates) == 0:\n",
    "                logger.info('{} No candidates found, try with {} columns next time'.format(table_name, MAX_COLUMNS + 1))\n",
    "        else:\n",
    "            logger.info('{} has {} rows.'.format(table_name, len(df_level_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#owner is table_schema \n",
    "owner = 'def'\n",
    "owner = conn_string[\"catalog\"]\n",
    "owner = conn_string[\"schema\"]\n",
    "for table in tqdm(table_list):\n",
    "    #print(table)\n",
    "    if table not in not_include and table not in processed_list:\n",
    "        try:\n",
    "            get_candidates(owner, table)\n",
    "        #except:\n",
    "        except Exception as e:\n",
    "            logger.info('{}: Error {}'.format(table, e))\n",
    "            pass\n",
    "        with open('tables_processed', 'a+') as f:\n",
    "            f.write('{}\\n'.format(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
